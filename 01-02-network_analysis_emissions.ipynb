{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ebe7b40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f9251b0",
   "metadata": {},
   "source": [
    "# Network Analysis with Emissions\n",
    "\n",
    "In this practice, we will use `codecarbon` [1] to track the CO2 emission during the execution of the code.\n",
    "\n",
    "\"CodeCarbon is a lightweight software package that seamlessly integrates into your Python codebase. It estimates the amount of carbon dioxide (CO2) produced by the cloud or personal computing resources used to execute the code.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5907fe4",
   "metadata": {},
   "source": [
    "## Example from social circles data\n",
    "\n",
    "This practice will add two tasks on top of the previous practice.\n",
    "\n",
    "- Find the longest and shortest path\n",
    "\n",
    "- Track the CO2 emission when executing the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7307e",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "https://snap.stanford.edu/data/ego-Facebook.html\n",
    "\n",
    "This dataset (*facebook_combined.txt*) consists of circles from Facebook (4,039 nodes and 88,234 edges) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d45a27",
   "metadata": {},
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import psutil, time, os, gc, statistics, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from codecarbon import EmissionsTracker\n",
    "from collections import defaultdict, deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f44fd2",
   "metadata": {},
   "source": [
    "### Example of Tracking emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48329bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon DEBUG @ 21:58:41] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 21:58:41] Energy consumed for RAM : 0.001812 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 21:58:41] Done measure for RAM - measurement time: 0.0019 s - last call 14.99 s\n",
      "[codecarbon INFO @ 21:58:41] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 21:58:41] Energy consumed for All CPU : 0.025681 kWh\n",
      "[codecarbon DEBUG @ 21:58:41] Done measure for CPU - measurement time: 0.0004 s - last call 15.00 s\n",
      "[codecarbon INFO @ 21:58:41] 0.027493 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 21:58:41] last_duration=14.994681833020877\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 21:58:56] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 21:58:56] Energy consumed for RAM : 0.001825 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 21:58:56] Done measure for RAM - measurement time: 0.0024 s - last call 15.00 s\n",
      "[codecarbon INFO @ 21:58:56] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 21:58:56] Energy consumed for All CPU : 0.025858 kWh\n",
      "[codecarbon DEBUG @ 21:58:56] Done measure for CPU - measurement time: 0.0020 s - last call 15.01 s\n",
      "[codecarbon INFO @ 21:58:56] 0.027683 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 21:58:56] last_duration=15.00232612498803\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 21:59:11] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 21:59:11] Energy consumed for RAM : 0.001837 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 21:59:11] Done measure for RAM - measurement time: 0.0023 s - last call 14.99 s\n",
      "[codecarbon INFO @ 21:59:11] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 21:59:11] Energy consumed for All CPU : 0.026035 kWh\n",
      "[codecarbon DEBUG @ 21:59:11] Done measure for CPU - measurement time: 0.0007 s - last call 15.00 s\n",
      "[codecarbon INFO @ 21:59:11] 0.027872 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 21:59:11] last_duration=14.994991540967021\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 21:59:26] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 21:59:26] Energy consumed for RAM : 0.001850 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 21:59:26] Done measure for RAM - measurement time: 0.0014 s - last call 15.00 s\n",
      "[codecarbon INFO @ 21:59:26] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 21:59:26] Energy consumed for All CPU : 0.026212 kWh\n",
      "[codecarbon DEBUG @ 21:59:26] Done measure for CPU - measurement time: 0.0005 s - last call 15.00 s\n",
      "[codecarbon INFO @ 21:59:26] 0.028062 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 21:59:26] last_duration=14.99661150004249\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 21:59:41] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 21:59:41] Energy consumed for RAM : 0.001862 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 21:59:41] Done measure for RAM - measurement time: 0.0036 s - last call 15.00 s\n",
      "[codecarbon INFO @ 21:59:41] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 21:59:41] Energy consumed for All CPU : 0.026389 kWh\n",
      "[codecarbon DEBUG @ 21:59:41] Done measure for CPU - measurement time: 0.0010 s - last call 15.01 s\n",
      "[codecarbon INFO @ 21:59:41] 0.028252 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 21:59:41] last_duration=15.002578084007837\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 21:59:56] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 21:59:56] Energy consumed for RAM : 0.001875 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 21:59:56] Done measure for RAM - measurement time: 0.0009 s - last call 14.99 s\n",
      "[codecarbon INFO @ 21:59:56] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 21:59:56] Energy consumed for All CPU : 0.026566 kWh\n",
      "[codecarbon DEBUG @ 21:59:56] Done measure for CPU - measurement time: 0.0005 s - last call 15.00 s\n",
      "[codecarbon INFO @ 21:59:56] 0.028441 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 21:59:56] last_duration=14.994857084006071\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 22:00:11] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 22:00:11] Energy consumed for RAM : 0.001887 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 22:00:11] Done measure for RAM - measurement time: 0.0026 s - last call 15.00 s\n",
      "[codecarbon INFO @ 22:00:11] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 22:00:11] Energy consumed for All CPU : 0.026743 kWh\n",
      "[codecarbon DEBUG @ 22:00:11] Done measure for CPU - measurement time: 0.0010 s - last call 15.01 s\n",
      "[codecarbon INFO @ 22:00:11] 0.028631 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 22:00:11] last_duration=15.003291666973382\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 22:00:26] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 22:00:26] Energy consumed for RAM : 0.001900 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 22:00:26] Done measure for RAM - measurement time: 0.0021 s - last call 15.00 s\n",
      "[codecarbon INFO @ 22:00:26] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 22:00:26] Energy consumed for All CPU : 0.026921 kWh\n",
      "[codecarbon DEBUG @ 22:00:26] Done measure for CPU - measurement time: 0.0007 s - last call 15.00 s\n",
      "[codecarbon INFO @ 22:00:26] 0.028820 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 22:00:26] We apply an energy mix of 238 g.CO2eq/kWh for United Kingdom\n",
      "[codecarbon DEBUG @ 22:00:26] EmissionsData(timestamp='2025-07-15T22:00:26', project_name='codecarbon', run_id='None', experiment_id='d960f128-b0b6-4eb2-a1e1-d2bbc81bbbe0', duration=2280.595489208994, emissions=0.00684741098927795, emissions_rate=3.0024662513267177e-06, cpu_power=42.5, gpu_power=0.0, ram_power=3.0, cpu_energy=0.026920533946325993, gpu_energy=0.0, ram_energy=0.0018998701518349128, energy_consumed=0.0288204040981609, country_name='United Kingdom', country_iso_code='GBR', region='england', cloud_provider='', cloud_region='', os='macOS-15.5-arm64-arm-64bit-Mach-O', python_version='3.13.5', codecarbon_version='3.0.2', cpu_count=10, cpu_model='Apple M4', gpu_count=None, gpu_model=None, longitude=-0.2211, latitude=51.5043, ram_total_size=16.0, tracking_mode='machine', on_cloud='N', pue=1.0)\n",
      "[codecarbon INFO @ 22:00:26] 0.003003 g.CO2eq/s mean an estimation of 94.68722533454849 kg.CO2eq/year\n",
      "[codecarbon WARNING @ 22:00:26] ApiClient.add_emission() need a run_id : the initial call may have failed. Retrying...\n",
      "[codecarbon ERROR @ 22:00:26] ApiClient Error when calling the API on https://api.codecarbon.io/runs with : {\"timestamp\": \"2025-07-15T22:00:26.895030+01:00\", \"experiment_id\": \"d960f128-b0b6-4eb2-a1e1-d2bbc81bbbe0\", \"os\": \"macOS-15.5-arm64-arm-64bit-Mach-O\", \"python_version\": \"3.13.5\", \"codecarbon_version\": \"3.0.2\", \"cpu_count\": 10, \"cpu_model\": \"Apple M4\", \"gpu_count\": null, \"gpu_model\": null, \"longitude\": -0.2, \"latitude\": 51.5, \"region\": null, \"provider\": null, \"ram_total_size\": 16.0, \"tracking_mode\": \"machine\"}\n",
      "[codecarbon ERROR @ 22:00:26] ApiClient API return http code 403 and answer : {\"detail\":\"Not allowed to perform this action\"}\n",
      "[codecarbon ERROR @ 22:00:26] ApiClient.add_emission still no run_id, aborting for this time !\n",
      "[codecarbon DEBUG @ 22:00:26] last_duration=14.996013958007097\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 22:00:41] RAM power estimation: 3.00W for 16.00GB\n",
      "[codecarbon INFO @ 22:00:41] Energy consumed for RAM : 0.001912 kWh. RAM Power : 3.0 W\n",
      "[codecarbon DEBUG @ 22:00:41] Done measure for RAM - measurement time: 0.0015 s - last call 15.00 s\n",
      "[codecarbon INFO @ 22:00:41] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
      "[codecarbon INFO @ 22:00:41] Energy consumed for All CPU : 0.027098 kWh\n",
      "[codecarbon DEBUG @ 22:00:41] Done measure for CPU - measurement time: 0.0006 s - last call 15.00 s\n",
      "[codecarbon INFO @ 22:00:41] 0.029010 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 22:00:41] last_duration=14.999325499986298\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the EmissionsTracker\n",
    "tracker = EmissionsTracker()\n",
    "\n",
    "# Start tracking emissions\n",
    "tracker.start()\n",
    "\n",
    "# Simulate a computational task (e.g., matrix multiplication)\n",
    "matrix_size = 1000\n",
    "A = np.random.rand(matrix_size, matrix_size)\n",
    "B = np.random.rand(matrix_size, matrix_size)\n",
    "result = np.matmul(A, B)\n",
    "\n",
    "# Simulate some processing time\n",
    "time.sleep(2)\n",
    "\n",
    "# Stop tracking and get emissions (in kgCO2)\n",
    "emissions = tracker.stop()\n",
    "\n",
    "# Print results\n",
    "print(f\"Emissions for matrix multiplication: {emissions:,.6f} kgCO2\")\n",
    "print(f\"Result matrix sum: {np.sum(result):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809f2f3",
   "metadata": {},
   "source": [
    "### Functions (need to be completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fbff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 / 1024  # Memory in MB\n",
    "\n",
    "\n",
    "\n",
    "# Naive implementation using adjacency list\n",
    "def analyze_naive(file_path):\n",
    "    start_time = time.time()\n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    # Build adjacency list\n",
    "    graph = defaultdict(set)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            n1, n2 = map(int, line.strip().split())\n",
    "            graph[n1].add(n2)\n",
    "            graph[n2].add(n1)\n",
    "    \n",
    "    num_nodes = len(graph)\n",
    "    num_edges = sum(len(neighbors) for neighbors in graph.values()) // 2\n",
    "    avg_degree = sum(len(neighbors) for neighbors in graph.values()) / num_nodes if num_nodes > 0 else 0\n",
    "    density = (2 * num_edges) / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "    \n",
    "    # Simple BFS for shortest path and tracking nodes\n",
    "    def bfs_shortest_path(start, end):\n",
    "        if start == end:\n",
    "            return 0, [start]\n",
    "        visited = {start}\n",
    "        queue = [(start, [start])]\n",
    "        while queue:\n",
    "            node, path = queue.pop(0)\n",
    "            for neighbor in graph[node]:\n",
    "                if neighbor not in visited:\n",
    "                    new_path = path + [neighbor]\n",
    "                    if neighbor == end:\n",
    "                        return len(new_path) - 1, new_path\n",
    "                    visited.add(neighbor)\n",
    "                    queue.append((neighbor, new_path))\n",
    "        return float('inf'), []\n",
    "    \n",
    "    # Find approximate longest and shortest paths\n",
    "    max_path = 0\n",
    "    min_path = float('inf')\n",
    "    max_path_nodes = []\n",
    "    min_path_nodes = []\n",
    "    sample_nodes = list(graph.keys())[:min(100, num_nodes)]  # Sample for efficiency\n",
    "    for i, start in enumerate(sample_nodes):\n",
    "        for end in sample_nodes[i+1:]:\n",
    "            path_len, path_nodes = bfs_shortest_path(start, end)\n",
    "            if path_len != float('inf'):\n",
    "                if path_len > max_path:\n",
    "                    max_path = path_len\n",
    "                    max_path_nodes = path_nodes\n",
    "                if path_len > 0 and path_len < min_path:\n",
    "                    min_path = path_len\n",
    "                    min_path_nodes = path_nodes\n",
    "    \n",
    "    # Find top 20 nodes by degree\n",
    "    degrees = [(node, len(neighbors)) for node, neighbors in graph.items()]\n",
    "    top_nodes = sorted(degrees, key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_nodes_dict = [{\"Node\": node, \"Degree\": degree} for node, degree in top_nodes]\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    memory_used = get_memory_usage() - start_memory\n",
    "    \n",
    "    print(f\"Nodes: {num_nodes}\")\n",
    "    print(f\"Edges: {num_edges}\")\n",
    "    print(f\"Average Degree: {avg_degree:,.2f}\")\n",
    "    print(f\"Density: {density:,.6f}\")\n",
    "    print(f\"Longest Path: {max_path}\")\n",
    "    print(f\"Longest Path Nodes: {max_path_nodes}\")\n",
    "    print(f\"Shortest Path: {min_path if min_path != float('inf') else 'N/A'}\")\n",
    "    print(f\"Shortest Path Nodes: {min_path_nodes if min_path != float('inf') else 'N/A'}\")\n",
    "    print(f\"Execution Time: {exec_time:,.2f} seconds\")\n",
    "    print(f\"Memory Used: {memory_used:,.2f} MB\")\n",
    "    print(\"Top 20 Most Connected Nodes:\")\n",
    "    for node in top_nodes_dict:\n",
    "        print(f\"  Node {node['Node']}: Degree {node['Degree']}\")\n",
    "    \n",
    "    return {\n",
    "        \"Method\": \"Naive (Adjacency List)\",\n",
    "        \"Nodes\": num_nodes,\n",
    "        \"Edges\": num_edges,\n",
    "        \"Avg Degree\": avg_degree,\n",
    "        \"Density\": density,\n",
    "        \"Longest Path\": max_path,\n",
    "        \"Longest Path Nodes\": max_path_nodes,\n",
    "        \"Shortest Path\": min_path if min_path != float('inf') else 0,\n",
    "        \"Shortest Path Nodes\": min_path_nodes if min_path != float('inf') else [],\n",
    "        \"Time (s)\": exec_time,\n",
    "        \"Memory (MB)\": memory_used,\n",
    "        \"Top 20 Nodes\": top_nodes_dict\n",
    "    }\n",
    "\n",
    "# Function to analyze full graph with NetworkX\n",
    "def analyze_networkx(file_path):\n",
    "    start_time = time.time() \n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    # TODO: Read the edge list into a NetworkX undirected graph\n",
    "    # - Use nx.read_edgelist with integer node types and undirected graph (nx.Graph())\n",
    "\n",
    "    # TODO: Compute basic graph metrics\n",
    "    # - Number of nodes: Use NetworkX method to count nodes\n",
    "    # - Number of edges: Use NetworkX method to count edges\n",
    "    # - Average degree: Sum of degrees divided by number of nodes\n",
    "    # - Density: Use formula (2 * num_edges) / (num_nodes * (num_nodes - 1)) (handle small graphs)\n",
    "    \n",
    "    # TODO: Find longest and shortest paths with their nodes\n",
    "    # - Use nx.all_pairs_shortest_path to get all shortest paths\n",
    "    # - For each path dictionary, compute path length (number of nodes - 1)\n",
    "    # - Track the maximum and minimum path lengths and their node lists\n",
    "    # - Ensure min_path is only updated for non-zero paths\n",
    "    # - Only compute paths if num_nodes > 1\n",
    "    \n",
    "    \n",
    "    # Find top 20 nodes by degree\n",
    "    degrees = G.degree()\n",
    "    top_nodes = sorted(degrees, key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_nodes_dict = [{\"Node\": node, \"Degree\": degree} for node, degree in top_nodes]\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    memory_used = get_memory_usage() - start_memory\n",
    "    \n",
    "    print(f\"Nodes: {num_nodes}\")\n",
    "    print(f\"Edges: {num_edges}\")\n",
    "    print(f\"Average Degree: {avg_degree:,.2f}\")\n",
    "    print(f\"Density: {density:,.6f}\")\n",
    "    print(f\"Longest Path: {longest_path}\")\n",
    "    print(f\"Longest Path Nodes: {longest_path_nodes}\")\n",
    "    print(f\"Shortest Path: {shortest_path if shortest_path != float('inf') else 'N/A'}\")\n",
    "    print(f\"Shortest Path Nodes: {shortest_path_nodes if shortest_path != float('inf') else 'N/A'}\")\n",
    "    print(f\"Execution Time: {exec_time:,.2f} seconds\")\n",
    "    print(f\"Memory Used: {memory_used:,.2f} MB\")\n",
    "    print(\"Top 20 Most Connected Nodes:\")\n",
    "    for node in top_nodes_dict:\n",
    "        print(f\"  Node {node['Node']}: Degree {node['Degree']}\")\n",
    "    \n",
    "    return {\n",
    "        \"Method\": \"NetworkX (Full)\",\n",
    "        \"Nodes\": num_nodes,\n",
    "        \"Edges\": num_edges,\n",
    "        \"Avg Degree\": avg_degree,\n",
    "        \"Density\": density,\n",
    "        \"Longest Path\": longest_path,\n",
    "        \"Longest Path Nodes\": longest_path_nodes,\n",
    "        \"Shortest Path\": shortest_path,\n",
    "        \"Shortest Path Nodes\": shortest_path_nodes,\n",
    "        \"Time (s)\": exec_time,\n",
    "        \"Memory (MB)\": memory_used,\n",
    "        \"Top 20 Nodes\": top_nodes_dict\n",
    "    }\n",
    "\n",
    "# SciPy sparse matrix analysis\n",
    "def analyze_scipy_sparse(file_path):\n",
    "    start_time = time.time()   \n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    # TODO: Load the edge list into a NumPy array\n",
    "    # - Use pandas.read_csv with space separator, no header, int32 dtype, and 'c' engine\n",
    "    # - Convert the DataFrame to a NumPy array\n",
    "    \n",
    "    # TODO: Create a sparse adjacency matrix\n",
    "    # - Get unique nodes and their indices (use np.unique with return_inverse=True)\n",
    "    # - Create row and column indices for a symmetric adjacency matrix (undirected graph)\n",
    "    # - Use sp.csr_matrix to create the sparse matrix with shape (num_nodes, num_nodes)\n",
    "    \n",
    "    \n",
    "    # TODO: Compute basic graph metrics\n",
    "    # - Number of edges: Use adj_matrix.nnz and divide by 2 (undirected graph)\n",
    "    # - Degrees: Sum the adjacency matrix along one axis\n",
    "    # - Average degree: Mean of the degrees\n",
    "    # - Density: Use formula (2 * num_edges) / (num_nodes * (num_nodes - 1)) (handle small graphs)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # TODO: Find longest and shortest paths with their nodes\n",
    "    # - Convert the sparse matrix to a NetworkX graph using nx.from_scipy_sparse_array\n",
    "    # - Use nx.all_pairs_shortest_path to get all shortest paths\n",
    "    # - For each path dictionary, compute path length (number of nodes - 1)\n",
    "    # - Track the maximum and minimum path lengths and their node lists\n",
    "    # - Ensure min_path is only updated for non-zero paths\n",
    "    # - Only compute paths if num_nodes > 1\n",
    "    \n",
    "    \n",
    "    # Find top 20 nodes by degree\n",
    "    top_indices = np.argpartition(degrees, -20)[-20:]\n",
    "    top_degrees = degrees[top_indices]\n",
    "    top_nodes = [(nodes[i], degrees[i]) for i in top_indices]\n",
    "    top_nodes = sorted(top_nodes, key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_nodes_dict = [{\"Node\": node, \"Degree\": degree} for node, degree in top_nodes]\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    memory_used = get_memory_usage() - start_memory\n",
    "    \n",
    "    print(f\"Nodes: {num_nodes}\")\n",
    "    print(f\"Edges: {num_edges}\")\n",
    "    print(f\"Average Degree: {avg_degree:,.2f}\")\n",
    "    print(f\"Density: {density:,.6f}\")\n",
    "    print(f\"Longest Path: {longest_path}\")\n",
    "    print(f\"Longest Path Nodes: {longest_path_nodes}\")\n",
    "    print(f\"Shortest Path: {shortest_path if shortest_path != float('inf') else 'N/A'}\")\n",
    "    print(f\"Shortest Path Nodes: {shortest_path_nodes if shortest_path != float('inf') else 'N/A'}\")\n",
    "    print(f\"Execution Time: {exec_time:,.2f} seconds\")\n",
    "    print(f\"Memory Used: {memory_used:,.2f} MB\")\n",
    "    print(\"Top 20 Most Connected Nodes:\")\n",
    "    for node in top_nodes_dict:\n",
    "        print(f\"  Node {node['Node']}: Degree {node['Degree']}\")\n",
    "    \n",
    "    return {\n",
    "        \"Method\": \"SciPy Sparse (Full)\",\n",
    "        \"Nodes\": num_nodes,\n",
    "        \"Edges\": num_edges,\n",
    "        \"Avg Degree\": avg_degree,\n",
    "        \"Density\": density,\n",
    "        \"Longest Path\": longest_path,\n",
    "        \"Longest Path Nodes\": longest_path_nodes,\n",
    "        \"Shortest Path\": shortest_path,\n",
    "        \"Shortest Path Nodes\": shortest_path_nodes,\n",
    "        \"Time (s)\": exec_time,\n",
    "        \"Memory (MB)\": memory_used,\n",
    "        \"Top 20 Nodes\": top_nodes_dict\n",
    "    }\n",
    "\n",
    "# Function to run analysis with multiple iterations\n",
    "def run_analysis_with_repeats(analysis_func, file_path, num_runs=5):\n",
    "    results = []\n",
    "    emissions = []\n",
    "    \n",
    "    # TODO: Run the analysis function multiple times with emission tracking\n",
    "    # - Loop num_runs times\n",
    "    # - Clear memory before each run using gc.collect()\n",
    "    # - Initialize an EmissionsTracker to measure carbon footprint\n",
    "    # - Start the tracker before running the analysis\n",
    "    # - Run the analysis function (analysis_func(file_path))\n",
    "    # - Stop the tracker and append the emissions (in kgCO2) to the emissions list\n",
    "    # - Append the analysis result to the results list\n",
    "    for _ in range(num_runs):\n",
    "        \n",
    "    \n",
    "    # Compute statistics\n",
    "    metrics = {\n",
    "        \"Method\": results[0][\"Method\"],\n",
    "        \"Nodes\": results[0][\"Nodes\"],\n",
    "        \"Edges\": results[0][\"Edges\"],\n",
    "        \"Avg Degree\": results[0][\"Avg Degree\"],\n",
    "        \"Density\": results[0][\"Density\"],\n",
    "        \"Longest Path\": results[0][\"Longest Path\"],\n",
    "        \"Longest Path Nodes\": results[0][\"Longest Path Nodes\"],\n",
    "        \"Shortest Path\": results[0][\"Shortest Path\"],\n",
    "        \"Shortest Path Nodes\": results[0][\"Shortest Path Nodes\"],\n",
    "        \"Time (s)\": statistics.mean([r[\"Time (s)\"] for r in results]),\n",
    "        \"Time Std (s)\": statistics.stdev([r[\"Time (s)\"] for r in results]) if num_runs > 1 else 0,\n",
    "        \"Memory (MB)\": statistics.mean([r[\"Memory (MB)\"] for r in results]),\n",
    "        \"Memory Std (MB)\": statistics.stdev([r[\"Memory (MB)\"] for r in results]) if num_runs > 1 else 0,\n",
    "        \"Emissions (kgCO2)\": statistics.mean(emissions),\n",
    "        \"Emissions Std (kgCO2)\": statistics.stdev(emissions) if num_runs > 1 else 0\n",
    "    }\n",
    "    \n",
    "    return metrics, results[0][\"Top 20 Nodes\"]  # Return metrics and top nodes from first run\n",
    "\n",
    "# Function to run all analyses\n",
    "def run_all_analyses(file_path, num_runs):\n",
    "    methods = [\n",
    "        (\"Naive\", analyze_naive),\n",
    "        (\"NetworkX\", analyze_networkx),\n",
    "        (\"SciPy Sparse\", analyze_scipy_sparse)\n",
    "    ]\n",
    "    results = []\n",
    "    top_nodes_dfs = []\n",
    "    \n",
    "    for method_name, method_func in methods:\n",
    "        print(f\"\\nRunning {method_name} analysis ({num_runs} runs)\")\n",
    "        metrics, top_nodes = run_analysis_with_repeats(method_func, file_path, num_runs)\n",
    "        results.append(metrics)\n",
    "        top_nodes_df = pd.DataFrame(top_nodes)\n",
    "        top_nodes_df['Method'] = f\"{method_name} (Full)\"\n",
    "        top_nodes_dfs.append(top_nodes_df)\n",
    "    \n",
    "    return results, top_nodes_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb6765",
   "metadata": {},
   "source": [
    "### Main function and combined output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65883b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    file_path = \"data/facebook_combined.txt\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: {file_path} not found. Please download from http://snap.stanford.edu/data/egnets-Facebook.html\")\n",
    "        return\n",
    "    \n",
    "    num_runs = 50\n",
    "    results, top_nodes_dfs = run_all_analyses(file_path, num_runs)\n",
    "    \n",
    "    # Combine results\n",
    "    print(\"\\nTable: Performance Comparison (Averaged over runs)\")\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.round(6))\n",
    "    df.to_csv(\"01-02-graph_analysis_comparison.csv\", index=False)\n",
    "    print(\"\\nResults saved to 01-02-graph_analysis_comparison.csv\")\n",
    "    \n",
    "    # Save top nodes to CSV\n",
    "    top_nodes_combined = pd.concat(top_nodes_dfs, ignore_index=True)\n",
    "    top_nodes_combined.to_csv(\"01-02-top_20_nodes.csv\", index=False)\n",
    "    print(\"Top 20 nodes saved to 01-02-top_20_nodes.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806df5c",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. https://codecarbon.io/#howitwork"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
