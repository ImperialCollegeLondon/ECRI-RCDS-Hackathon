{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ebe7b40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f9251b0",
   "metadata": {},
   "source": [
    "# Network Analysis with Profiling\n",
    "\n",
    "In this example, we aim to use 3 different methods to calculate the number of nodes, the number of edges, the number of triangles, the everage degree, and the density of the network, find the top 20 most connected nodes and also track the CPU runtime and the use of RAM.\n",
    "\n",
    "We will use a loop-based naïve method, a networkx-based method and a scipy sparse network based method to complete the above tasks and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5907fe4",
   "metadata": {},
   "source": [
    "## Example from social circles data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7307e",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "https://snap.stanford.edu/data/ego-Facebook.html\n",
    "\n",
    "This dataset (*facebook_combined.txt*) consists of circles from Facebook (4,039 nodes and 88,234 edges) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ef72a",
   "metadata": {},
   "source": [
    "### Profiling method for monitoring functions\n",
    "\n",
    "Profiling by function gives us a high-level idea of how often functions are called and how long those calls last. One way to do this is to import the ```cProfile``` module and run a function using the ```cProfile.run()``` function, providing a string argument which is the command used to invoke the function. ```cProfile``` is part of the Python standard library and so is available without installing any additional packages. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cProfile module\n",
    "import cProfile\n",
    "\n",
    "def is_even(value):\n",
    "  if value%2 == 0:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def halve(value):\n",
    "  return value / 2\n",
    "\n",
    "def function_to_test(upper_value):\n",
    "  result = 0\n",
    "  for i in range(int(upper_value) +1):\n",
    "    if is_even(i):\n",
    "      result=result + halve(i)\n",
    "\n",
    "  print(result)\n",
    "\n",
    "# Call the cProfile.run() method with a string argument that is the call to the function you want to profile\n",
    "cProfile.run('function_to_test(1e7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb84b3",
   "metadata": {},
   "source": [
    "The results show the total time spent running the code and the total number of function calls. Then, for each function, it shows:\n",
    "* ```ncalls```: the number of times the function was called.\n",
    "* ```tottime```: the total time spent in the function, excluding time spent in functions called by the function.\n",
    "* ```percall```: the time spent in the function per call, excluding time spent in functions called by the function.\n",
    "* ```cumtime```: the total time spent in the function, including time spent in functions called by the function.\n",
    "* ```percall```: the time spent in the function per call, including time spent in functions called by the function.\n",
    "* ```filename:lineno(function)```: the filename, line number and function name.\n",
    "\n",
    "There will normally be a number of functions which are not functions you are written or explicitly called. These are often called as part of how Python internally executes your code. They are normally not very consequential in terms of run-time and can often be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d45a27",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "By implementing the profiling for each of the network analysis functions, observe the outcome and understand the difference in performance of each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import psutil, time, os, gc, statistics, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import the cProfile module\n",
    "import cProfile\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809f2f3",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1426740",
   "metadata": {},
   "source": [
    "#### Tracking memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fbff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 / 1024  # Memory in MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0f6bc",
   "metadata": {},
   "source": [
    "#### Three different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Naive method using dictionary-based adjacency list\n",
    "def analyze_naive(file_path):\n",
    "    \"\"\"\n",
    "    A naïve method to analyze number of vertices (nodes), edges, triangles and most connected nodes.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the input file.\n",
    "        \n",
    "    \n",
    "    Returns:\n",
    "        results is a list of metric dicts, and top_nodes_dfs is a list of DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    # Initialize adjacency list as a dictionary\n",
    "    graph = {}\n",
    "    \n",
    "    # Read edge list and build graph\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    node1, node2 = map(int, line.strip().split())\n",
    "                    # Add nodes and edges (undirected)\n",
    "                    if node1 not in graph:\n",
    "                        graph[node1] = set()\n",
    "                    if node2 not in graph:\n",
    "                        graph[node2] = set()\n",
    "                    graph[node1].add(node2)\n",
    "                    graph[node2].add(node1)\n",
    "    except (ValueError, IOError) as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return {\n",
    "            \"Method\": \"Naive (Full)\",\n",
    "            \"Nodes\": 0,\n",
    "            \"Edges\": 0,\n",
    "            \"Triangles\": 0,\n",
    "            \"Avg Degree\": 0,\n",
    "            \"Density\": 0,\n",
    "            \"Time (s)\": time.time() - start_time,\n",
    "            \"Memory (MB)\": get_memory_usage() - start_memory,\n",
    "            \"Top 20 Nodes\": []\n",
    "        }\n",
    "    \n",
    "    num_nodes = len(graph)\n",
    "    num_edges = sum(len(neighbors) for neighbors in graph.values()) // 2  # Divide by 2 for undirected\n",
    "    \n",
    "    # Count triangles, ensuring each triangle is counted once\n",
    "    num_triangles = 0\n",
    "    for node in graph:\n",
    "        neighbors = sorted(graph[node])  # Sort neighbors for consistent ordering\n",
    "        for i, n1 in enumerate(neighbors):\n",
    "            if n1 < node:  # Only count triangles where node is not the smallest\n",
    "                continue\n",
    "            for n2 in neighbors[i+1:]:\n",
    "                if n2 < node:  # Ensure node is the smallest in the triangle\n",
    "                    continue\n",
    "                if n2 in graph[n1]:  # Check if n1 and n2 are connected\n",
    "                    num_triangles += 1\n",
    "    \n",
    "    avg_degree = sum(len(neighbors) for neighbors in graph.values()) / num_nodes if num_nodes > 0 else 0\n",
    "    density = (2 * num_edges) / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "    \n",
    "    # Find top 20 nodes by degree\n",
    "    top_nodes = sorted(graph.items(), key=lambda x: len(x[1]), reverse=True)[:20]\n",
    "    top_nodes_dict = [{\"Node\": node, \"Degree\": len(neighbors)} for node, neighbors in top_nodes]\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    memory_used = get_memory_usage() - start_memory\n",
    "    \n",
    "    print(f\"Nodes: {num_nodes}\")\n",
    "    print(f\"Edges: {num_edges}\")\n",
    "    print(f\"Triangles: {num_triangles}\")\n",
    "    print(f\"Average Degree: {avg_degree:,.2f}\")\n",
    "    print(f\"Density: {density:,.6f}\")\n",
    "    print(f\"Execution Time: {exec_time:,.2f} seconds\")\n",
    "    print(f\"Memory Used: {memory_used:,.2f} MB\")\n",
    "    print(\"Top 20 Most Connected Nodes:\")\n",
    "    for node in top_nodes_dict:\n",
    "        print(f\"  Node {node['Node']}: Degree {node['Degree']}\")\n",
    "    \n",
    "    return {\n",
    "        \"Method\": \"Naive (Full)\",\n",
    "        \"Nodes\": num_nodes,\n",
    "        \"Edges\": num_edges,\n",
    "        \"Triangles\": num_triangles,\n",
    "        \"Avg Degree\": avg_degree,\n",
    "        \"Density\": density,\n",
    "        \"Time (s)\": exec_time,\n",
    "        \"Memory (MB)\": memory_used,\n",
    "        \"Top 20 Nodes\": top_nodes_dict\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to analyze full graph with NetworkX\n",
    "def analyze_networkx(file_path):\n",
    "    start_time = time.time() \n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    try:\n",
    "        G = nx.read_edgelist(file_path, nodetype=int, create_using=nx.Graph())\n",
    "    except (nx.NetworkXError, IOError) as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return {\n",
    "            \"Method\": \"NetworkX (Full)\",\n",
    "            \"Nodes\": 0,\n",
    "            \"Edges\": 0,\n",
    "            \"Triangles\": 0,\n",
    "            \"Avg Degree\": 0,\n",
    "            \"Density\": 0,\n",
    "            \"Time (s)\": time.time() - start_time,\n",
    "            \"Memory (MB)\": get_memory_usage() - start_memory,\n",
    "            \"Top 20 Nodes\": []\n",
    "        }\n",
    "    \n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    num_triangles = sum(nx.triangles(G).values()) // 3  # Each triangle counted thrice\n",
    "    avg_degree = sum(dict(G.degree()).values()) / num_nodes if num_nodes > 0 else 0\n",
    "    density = (2 * num_edges) / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "    \n",
    "    # Find top 20 nodes by degree\n",
    "    degrees = G.degree()\n",
    "    top_nodes = sorted(degrees, key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_nodes_dict = [{\"Node\": node, \"Degree\": degree} for node, degree in top_nodes]\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    memory_used = get_memory_usage() - start_memory\n",
    "    \n",
    "    print(f\"Nodes: {num_nodes}\")\n",
    "    print(f\"Edges: {num_edges}\")\n",
    "    print(f\"Triangles: {num_triangles}\")\n",
    "    print(f\"Average Degree: {avg_degree:,.2f}\")\n",
    "    print(f\"Density: {density:,.6f}\")\n",
    "    print(f\"Execution Time: {exec_time:,.2f} seconds\")\n",
    "    print(f\"Memory Used: {memory_used:,.2f} MB\")\n",
    "    print(\"Top 20 Most Connected Nodes:\")\n",
    "    for node in top_nodes_dict:\n",
    "        print(f\"  Node {node['Node']}: Degree {node['Degree']}\")\n",
    "    \n",
    "    return {\n",
    "        \"Method\": \"NetworkX (Full)\",\n",
    "        \"Nodes\": num_nodes,\n",
    "        \"Edges\": num_edges,\n",
    "        \"Triangles\": num_triangles,\n",
    "        \"Avg Degree\": avg_degree,\n",
    "        \"Density\": density,\n",
    "        \"Time (s)\": exec_time,\n",
    "        \"Memory (MB)\": memory_used,\n",
    "        \"Top 20 Nodes\": top_nodes_dict\n",
    "    }\n",
    "\n",
    "# SciPy sparse matrix analysis\n",
    "def analyze_scipy_sparse(file_path):\n",
    "    start_time = time.time()   \n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    try:\n",
    "        # Load edge list with pandas for better performance\n",
    "        edges_df = pd.read_csv(file_path, sep='\\s+', header=None, dtype=np.int32, engine='c')\n",
    "        edges = edges_df.to_numpy()\n",
    "    except (pd.errors.EmptyDataError, IOError) as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return {\n",
    "            \"Method\": \"SciPy Sparse (Full)\",\n",
    "            \"Nodes\": 0,\n",
    "            \"Edges\": 0,\n",
    "            \"Triangles\": 0,\n",
    "            \"Avg Degree\": 0,\n",
    "            \"Density\": 0,\n",
    "            \"Time (s)\": time.time() - start_time,\n",
    "            \"Memory (MB)\": get_memory_usage() - start_memory,\n",
    "            \"Top 20 Nodes\": []\n",
    "        }\n",
    "    \n",
    "    # Vectorized node mapping\n",
    "    nodes, inverse_indices = np.unique(edges, return_inverse=True)\n",
    "    num_nodes = len(nodes)\n",
    "    edge_indices = inverse_indices.reshape(edges.shape)  # Shape: (m, 2)\n",
    "    \n",
    "    # Create row and column arrays for symmetric adjacency matrix\n",
    "    rows = np.concatenate([edge_indices[:, 0], edge_indices[:, 1]])\n",
    "    cols = np.concatenate([edge_indices[:, 1], edge_indices[:, 0]])\n",
    "    data = np.ones(len(rows), dtype=np.int32)\n",
    "    \n",
    "    adj_matrix = sp.csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n",
    "    \n",
    "    num_edges = adj_matrix.nnz // 2\n",
    "    degrees = np.array(adj_matrix.sum(axis=1)).flatten()\n",
    "    # Count triangles: trace(A^3)/6 for undirected graph\n",
    "    adj_matrix_cube = adj_matrix @ adj_matrix @ adj_matrix\n",
    "    num_triangles = int(adj_matrix_cube.diagonal().sum() / 6)\n",
    "    avg_degree = degrees.mean() if num_nodes > 0 else 0\n",
    "    density = (2 * num_edges) / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "    \n",
    "    # Find top 20 nodes by degree\n",
    "    top_indices = np.argpartition(degrees, -20)[-20:] if num_nodes >= 20 else np.arange(num_nodes)\n",
    "    top_degrees = degrees[top_indices]\n",
    "    top_nodes = [(nodes[i], degrees[i]) for i in top_indices]\n",
    "    top_nodes = sorted(top_nodes, key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_nodes_dict = [{\"Node\": node, \"Degree\": degree} for node, degree in top_nodes]\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    memory_used = get_memory_usage() - start_memory\n",
    "    \n",
    "    print(f\"Nodes: {num_nodes}\")\n",
    "    print(f\"Edges: {num_edges}\")\n",
    "    print(f\"Triangles: {num_triangles}\")\n",
    "    print(f\"Average Degree: {avg_degree:,.2f}\")\n",
    "    print(f\"Density: {density:,.6f}\")\n",
    "    print(f\"Execution Time: {exec_time:,.2f} seconds\")\n",
    "    print(f\"Memory Used: {memory_used:,.2f} MB\")\n",
    "    print(\"Top 20 Most Connected Nodes:\")\n",
    "    for node in top_nodes_dict:\n",
    "        print(f\"  Node {node['Node']}: Degree {node['Degree']}\")\n",
    "    \n",
    "    return {\n",
    "        \"Method\": \"SciPy Sparse (Full)\",\n",
    "        \"Nodes\": num_nodes,\n",
    "        \"Edges\": num_edges,\n",
    "        \"Triangles\": num_triangles,\n",
    "        \"Avg Degree\": avg_degree,\n",
    "        \"Density\": density,\n",
    "        \"Time (s)\": exec_time,\n",
    "        \"Memory (MB)\": memory_used,\n",
    "        \"Top 20 Nodes\": top_nodes_dict\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c2642",
   "metadata": {},
   "source": [
    "#### Functions to run through the different methods one-by-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c510b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def profile_wrapper_naive(file_path, num_runs):\n",
    "    \"\"\"\n",
    "    Wrapper to profile analyze_naive over num_runs and aggregate results.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the input file.\n",
    "        num_runs (int): Number of runs for averaging metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (results, top_nodes_dfs) where results is a list of metric dicts,\n",
    "               and top_nodes_dfs is a list of DataFrames.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    top_nodes_dfs = []\n",
    "    \n",
    "    # TODO: Run the analyze_naive function multiple times\n",
    "    # - Loop num_runs times\n",
    "    # - In each iteration, call analyze_naive(file_path) to get the result\n",
    "    # - Extract metrics (all keys except \"Top 20 Nodes\") into a dictionary\n",
    "    # - Convert the \"Top 20 Nodes\" list into a pandas DataFrame\n",
    "    # - Add a 'Method' column to the DataFrame with value \"Naive (Full)\"\n",
    "    # - Append the metrics dictionary to results and the DataFrame to top_nodes_dfs\n",
    "    \n",
    "    # TODO: Aggregate metrics across all runs\n",
    "    # - Check if results is not empty\n",
    "    # - Create a dictionary avg_result with:\n",
    "    #   - \"Method\": \"Naive (Full)\"\n",
    "    #   - Graph metrics (\"Nodes\", \"Edges\", \"Triangles\", \"Avg Degree\", \"Density\") from the first result\n",
    "    #   - \"Time (s)\": Average of \"Time (s)\" across all runs (use np.mean)\n",
    "    #   - \"Memory (MB)\": Average of \"Memory (MB)\" across all runs (use np.mean)\n",
    "    # - Return a list containing only the avg_result dictionary\n",
    "    # - If results is empty, return empty lists\n",
    "    if results:\n",
    "        avg_result = { # Fill this dictionary with metrics            \n",
    "        }\n",
    "        results = [avg_result]  \n",
    "    \n",
    "    return results, top_nodes_dfs\n",
    "\n",
    "# TODO: Implement the profile_wrapper_networkx function\n",
    "# - This function should profile analyze_networkx over num_runs and aggregate results.\n",
    "# - Follow the same structure as profile_wrapper_naive:\n",
    "\n",
    "def profile_wrapper_networkx(file_path, num_runs):\n",
    "    \n",
    "    \n",
    "    return results, top_nodes_dfs\n",
    "\n",
    "# TODO: Implement the profile_wrapper_scipy_sparse function\n",
    "# - This function should profile analyze_scipy_sparse over num_runs and aggregate results.\n",
    "# - Follow the same structure as profile_wrapper_naive:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ee505",
   "metadata": {},
   "source": [
    "### Test the profiling methods to observe the performance of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc584d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "        (\"Naive\", analyze_naive),\n",
    "        (\"NetworkX\", analyze_networkx),\n",
    "        (\"SciPy Sparse\", analyze_scipy_sparse)\n",
    "    ]\n",
    "file_path = \"data/facebook_combined.txt\"\n",
    "num_runs = 1\n",
    "# call naive method\n",
    "method_name, method_func = methods[0]\n",
    "print(f\"Profiling {method_name} method with {num_runs} runs on {file_path}\")\n",
    "cProfile.run(f'profile_wrapper_naive(\"{file_path}\", {num_runs})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "        (\"Naive\", analyze_naive),\n",
    "        (\"NetworkX\", analyze_networkx),\n",
    "        (\"SciPy Sparse\", analyze_scipy_sparse)\n",
    "    ]\n",
    "file_path = \"data/facebook_combined.txt\"\n",
    "num_runs = 1\n",
    "# call naive method\n",
    "method_name, method_func = methods[1]\n",
    "print(f\"Profiling {method_name} method with {num_runs} runs on {file_path}\")\n",
    "cProfile.run(f'profile_wrapper_networkx(\"{file_path}\", {num_runs})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806df5c",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
